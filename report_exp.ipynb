{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca6057f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[GpuDevice(id=0, process_index=0), GpuDevice(id=1, process_index=0), GpuDevice(id=2, process_index=0), GpuDevice(id=3, process_index=0)]\n"
     ]
    }
   ],
   "source": [
    "import torch, jax; print(torch.cuda.is_available()); print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d85968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as relu\n",
    "from jax.config import config\n",
    "from jax import jit, grad, random\n",
    "from jax import grad, jit, vmap, hessian, jvp, lax\n",
    "jnp.set_printoptions(suppress=True)\n",
    "key = random.PRNGKey(1)\n",
    "\n",
    "\n",
    "sigma_w = 1.5  # hidden layer variance\n",
    "sigma_v = 1 # output layer variance\n",
    "n_h = 100000 # Hidden-layer size\n",
    "d =  10# dimension of features\n",
    "N = 250  # Number of data vectors\n",
    "\n",
    "W = sigma_w/jnp.sqrt(n_h)  * random.normal(key, shape = (d, n_h))\n",
    "key, split = random.split(key)\n",
    "v = sigma_v * random.normal(split, shape =(n_h, 1))\n",
    "\n",
    "W_init = W\n",
    "v_init = v\n",
    "\n",
    "\n",
    "key, split = random.split(key)\n",
    "X_train = random.normal(split, shape = (N, d))\n",
    "# X_train = ( X_train.T / jnp.linalg.norm(X_train, axis = 1) ).T\n",
    "key, split = random.split(key)\n",
    "X_test = random.normal(split, shape = (N, d))\n",
    "# X_test = ( X_test.T / jnp.linalg.norm(X_test, axis = 1) ).T\n",
    "# print( jnp.linalg.norm(X_test, axis = 1))\n",
    "\n",
    "\n",
    "key, split = random.split(key)\n",
    "gr_truth = (jnp.arange(d) + 1) / d\n",
    "Y_train = jnp.dot(X_train, gr_truth) + 0.125 * random.normal(split, shape = (N,))\n",
    "key, split = random.split(key)\n",
    "Y_test = jnp.dot(X_test, gr_truth)  + 0.125 * random.normal(split, shape = (N,))\n",
    "\n",
    "data_train = (X_train,Y_train)\n",
    "data_test = (X_test,Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee422dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x, l):\n",
    "    \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
    "    return jnp.maximum(l*x, x)\n",
    "\n",
    "jit_ReLU = jit(ReLU)\n",
    "\n",
    "def forward_dyn(params, features, l):\n",
    "  v, W = params\n",
    "  batch_size = features.shape[0]\n",
    "  h = jit_ReLU( jnp.dot(features, W), l  )\n",
    "  # h = 0.5 * (h + jnp.abs(h))\n",
    "  return jnp.dot(h, v).reshape(batch_size)\n",
    "\n",
    "\n",
    "\n",
    "def lin_forward_dyn(params, features, l):\n",
    "  v, W = params\n",
    "  diff_v, diff_W = v - v_init, W - W_init\n",
    "  _preds1 = forward_dyn((v, W), features, l)\n",
    "  _preds2 = forward_dyn((diff_v, diff_W), features, l)\n",
    "  return _preds1 - _preds2,  _preds2 # Return the output of linearized netwrok, and linearization error\n",
    "\n",
    "\n",
    "\n",
    "def loss(params, data, l):\n",
    "  X, Y = data\n",
    "  preds = forward_dyn(params, X, l)\n",
    "  # print(preds.shape)\n",
    "  # print(Y.shape)\n",
    "  return 0.5 * jnp.mean( jnp.square( Y - preds) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hvp(loss_fn, params, v):\n",
    "  return  jvp(grad(loss_fn), [params], [v])[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "935a94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lanczos_helper(w, z, z_old, vecs, beta):\n",
    "    # print(w.shape)\n",
    "    # print(z.shape)\n",
    "    # print(vecs.shape)\n",
    "        \n",
    "    alpha  = jnp.dot(w,z)        # print(alpha)\n",
    "     \n",
    "    w     = w - alpha *z - beta * z_old\n",
    "        \n",
    "    coeffs = jnp.dot(vecs, w)\n",
    "    w = w - jnp.dot(coeffs, vecs)\n",
    "        \n",
    "    beta = jnp.linalg.norm(w)\n",
    "    return w/beta, alpha, beta\n",
    "\n",
    "def lanczos_alg(loss_fn, params,  order, split):\n",
    "    v, W = params\n",
    "    dim = v.shape[0] + W.shape[0]* W.shape[1]\n",
    "    tridiag = jnp.zeros(shape=(order+1,order+1))\n",
    "    # print(tridiag)\n",
    "    vecs = jnp.zeros(shape=(order+1, dim))\n",
    "\n",
    "    init_vec = random.laplace(split, shape=(dim,))\n",
    "    init_vec = init_vec / jnp.linalg.norm(init_vec)\n",
    "    vecs = vecs.at[0].set(init_vec) \n",
    "    z_old = 0\n",
    "    beta = 0\n",
    "    # print('Dim', dim)\n",
    "    for i in range(order):\n",
    "        z = vecs[i,:]\n",
    "        vpart = z[:v.shape[0]].reshape(v.shape[0], 1)\n",
    "        Wpart = z[v.shape[0]:].reshape(W.shape[0], W.shape[1])\n",
    "\n",
    "        vpart, Wpart = hvp(loss_fn, params, (vpart, Wpart))\n",
    "        w = jnp.concatenate( (vpart.flatten(), Wpart.flatten()) )\n",
    "        \n",
    "        \n",
    "        w, alpha, beta = jit(_lanczos_helper)(w, z, z_old, vecs, beta)\n",
    "        \n",
    "        if(beta < 1):\n",
    "          break\n",
    "        tridiag = tridiag.at[(i,i)].set(alpha)   \n",
    "        z_old = z\n",
    "        \n",
    "    \n",
    "        tridiag = tridiag.at[(i,i+1)].set(beta)\n",
    "        tridiag = tridiag.at[(i+1,i)].set(beta) \n",
    "        vecs = vecs.at[i+1].set(w)\n",
    "\n",
    "    # print(jnp.dot(vecs,vecs.T))\n",
    "    return tridiag[:i, :i], vecs[:i, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00ff2c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim: 1100000\n"
     ]
    }
   ],
   "source": [
    "T =  2000\n",
    "dim = v.shape[0] + W.shape[0]* W.shape[1]\n",
    "print(\"Dim:\", dim)\n",
    "l_vec = [0, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "freq = 20\n",
    "Hg_product_vec   = jnp.zeros( shape = (len(l_vec), T//freq) )\n",
    "overlap_vec      = jnp.zeros( shape = (len(l_vec), T//freq) )\n",
    "eig_vecs_diffs   = jnp.zeros( shape = (len(l_vec), T//freq, 3) )\n",
    "test_loss_vec    = jnp.zeros( shape = (len(l_vec), T//freq) )\n",
    "train_loss_vec   = jnp.zeros( shape = (len(l_vec), T//freq) )\n",
    "eig_vals_vec     = jnp.zeros( shape = (len(l_vec), T//freq, 100) )\n",
    "lin_err_vec      = jnp.zeros( shape = (len(l_vec),  T//freq) )\n",
    "rel_diff_vec = jnp.zeros(shape = (len(l_vec),) )\n",
    "diff_vec = jnp.zeros(shape = (len(l_vec),) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b159a878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leak: 0\n",
      "Train loss: 4.928611 Test loss: 5.0729313 Linearization Err: 0.0\n",
      "Train loss: 2.2293527 Test loss: 2.4990025 Linearization Err: 1.01607505e-08\n",
      "Train loss: 1.22905 Test loss: 1.528849 Linearization Err: 2.1226572e-08\n",
      "Train loss: 0.81985444 Test loss: 1.1263728 Linearization Err: 2.8292204e-08\n",
      "Train loss: 0.6269091 Test loss: 0.93383527 Linearization Err: 3.2829313e-08\n",
      "Train loss: 0.5188389 Test loss: 0.823931 Linearization Err: 3.584963e-08\n",
      "Train loss: 0.4476239 Test loss: 0.74971205 Linearization Err: 3.7964885e-08\n",
      "Train loss: 0.3947232 Test loss: 0.69304055 Linearization Err: 3.9540332e-08\n",
      "Train loss: 0.35242164 Test loss: 0.64643383 Linearization Err: 4.0766967e-08\n",
      "Train loss: 0.31716833 Test loss: 0.60651135 Linearization Err: 4.1758998e-08\n",
      "Train loss: 0.28711632 Test loss: 0.57155126 Linearization Err: 4.2586713e-08\n",
      "Train loss: 0.26115823 Test loss: 0.54055095 Linearization Err: 4.329604e-08\n",
      "Train loss: 0.23855029 Test loss: 0.5128439 Linearization Err: 4.3916035e-08\n",
      "Train loss: 0.2187439 Test loss: 0.48794705 Linearization Err: 4.446706e-08\n",
      "Train loss: 0.20131089 Test loss: 0.46548355 Linearization Err: 4.4960505e-08\n",
      "Train loss: 0.18590635 Test loss: 0.4451446 Linearization Err: 4.540807e-08\n",
      "Train loss: 0.1722454 Test loss: 0.42667398 Linearization Err: 4.5817217e-08\n",
      "Train loss: 0.16009094 Test loss: 0.4098572 Linearization Err: 4.6193627e-08\n",
      "Train loss: 0.14924335 Test loss: 0.3945093 Linearization Err: 4.6540833e-08\n",
      "Train loss: 0.13953352 Test loss: 0.38047045 Linearization Err: 4.6862127e-08\n",
      "Train loss: 0.13081594 Test loss: 0.367601 Linearization Err: 4.7159887e-08\n",
      "Train loss: 0.12296755 Test loss: 0.35578024 Linearization Err: 4.743648e-08\n",
      "Train loss: 0.11588343 Test loss: 0.34490278 Linearization Err: 4.76935e-08\n",
      "Train loss: 0.10947139 Test loss: 0.33487496 Linearization Err: 4.793273e-08\n",
      "Train loss: 0.10365311 Test loss: 0.32561502 Linearization Err: 4.8156764e-08\n",
      "Train loss: 0.09836042 Test loss: 0.31704903 Linearization Err: 4.836597e-08\n",
      "Train loss: 0.093534246 Test loss: 0.30911413 Linearization Err: 4.8561695e-08\n",
      "Train loss: 0.089123 Test loss: 0.30175206 Linearization Err: 4.8745424e-08\n",
      "Train loss: 0.08508185 Test loss: 0.2949126 Linearization Err: 4.89182e-08\n",
      "Train loss: 0.08137125 Test loss: 0.2885496 Linearization Err: 4.9081194e-08\n",
      "Train loss: 0.077956654 Test loss: 0.28262174 Linearization Err: 4.923404e-08\n",
      "Train loss: 0.074808046 Test loss: 0.27709278 Linearization Err: 4.9378187e-08\n",
      "Train loss: 0.07189866 Test loss: 0.27192965 Linearization Err: 4.9514007e-08\n",
      "Train loss: 0.069204666 Test loss: 0.26710242 Linearization Err: 4.9641823e-08\n",
      "Train loss: 0.06670519 Test loss: 0.262584 Linearization Err: 4.9762455e-08\n",
      "Train loss: 0.06438182 Test loss: 0.25834987 Linearization Err: 4.9876377e-08\n",
      "Train loss: 0.06221795 Test loss: 0.25437865 Linearization Err: 4.9984568e-08\n",
      "Train loss: 0.060198884 Test loss: 0.25064993 Linearization Err: 5.0087053e-08\n",
      "Train loss: 0.05831184 Test loss: 0.24714585 Linearization Err: 5.01841e-08\n",
      "Train loss: 0.05654484 Test loss: 0.2438492 Linearization Err: 5.0275847e-08\n",
      "Train loss: 0.054887187 Test loss: 0.24074504 Linearization Err: 5.0363195e-08\n",
      "Train loss: 0.053329654 Test loss: 0.23781943 Linearization Err: 5.0446324e-08\n",
      "Train loss: 0.05186365 Test loss: 0.23505998 Linearization Err: 5.0525458e-08\n",
      "Train loss: 0.050481692 Test loss: 0.23245502 Linearization Err: 5.0600867e-08\n",
      "Train loss: 0.049176812 Test loss: 0.22999372 Linearization Err: 5.067259e-08\n",
      "Train loss: 0.047942903 Test loss: 0.2276665 Linearization Err: 5.0740987e-08\n",
      "Train loss: 0.04677423 Test loss: 0.22546442 Linearization Err: 5.0805816e-08\n",
      "Train loss: 0.04566566 Test loss: 0.22337882 Linearization Err: 5.0867804e-08\n",
      "Train loss: 0.0446126 Test loss: 0.22140191 Linearization Err: 5.0927255e-08\n",
      "Train loss: 0.043610778 Test loss: 0.21952696 Linearization Err: 5.0983914e-08\n",
      "Train loss: 0.0426564 Test loss: 0.21774745 Linearization Err: 5.1038153e-08\n",
      "Train loss: 0.04174611 Test loss: 0.21605723 Linearization Err: 5.108994e-08\n",
      "Train loss: 0.04087665 Test loss: 0.21445093 Linearization Err: 5.11395e-08\n",
      "Train loss: 0.040045075 Test loss: 0.21292306 Linearization Err: 5.1186763e-08\n",
      "Train loss: 0.03924866 Test loss: 0.21146882 Linearization Err: 5.1232075e-08\n",
      "Train loss: 0.038485207 Test loss: 0.21008436 Linearization Err: 5.1275713e-08\n",
      "Train loss: 0.037752427 Test loss: 0.20876497 Linearization Err: 5.1317524e-08\n",
      "Train loss: 0.037048243 Test loss: 0.20750724 Linearization Err: 5.1357716e-08\n",
      "Train loss: 0.036370784 Test loss: 0.20630695 Linearization Err: 5.1396434e-08\n",
      "Train loss: 0.03571837 Test loss: 0.20516106 Linearization Err: 5.1433695e-08\n",
      "Train loss: 0.035089403 Test loss: 0.20406663 Linearization Err: 5.1469627e-08\n",
      "Train loss: 0.034482468 Test loss: 0.20302045 Linearization Err: 5.1504287e-08\n",
      "Train loss: 0.033896193 Test loss: 0.20202003 Linearization Err: 5.15374e-08\n",
      "Train loss: 0.033329308 Test loss: 0.2010627 Linearization Err: 5.156917e-08\n",
      "Train loss: 0.03278073 Test loss: 0.20014623 Linearization Err: 5.1599816e-08\n",
      "Train loss: 0.03224943 Test loss: 0.19926825 Linearization Err: 5.1629293e-08\n",
      "Train loss: 0.031734403 Test loss: 0.19842671 Linearization Err: 5.1657565e-08\n",
      "Train loss: 0.031234821 Test loss: 0.19761965 Linearization Err: 5.1684957e-08\n",
      "Train loss: 0.030749857 Test loss: 0.19684562 Linearization Err: 5.1711382e-08\n",
      "Train loss: 0.03027875 Test loss: 0.19610286 Linearization Err: 5.173682e-08\n",
      "Train loss: 0.029820714 Test loss: 0.19538951 Linearization Err: 5.1761383e-08\n",
      "Train loss: 0.029375091 Test loss: 0.19470423 Linearization Err: 5.1785044e-08\n",
      "Train loss: 0.028941305 Test loss: 0.19404565 Linearization Err: 5.1807817e-08\n",
      "Train loss: 0.028518718 Test loss: 0.19341227 Linearization Err: 5.1829723e-08\n",
      "Train loss: 0.028106809 Test loss: 0.19280268 Linearization Err: 5.1850893e-08\n",
      "Train loss: 0.027705086 Test loss: 0.19221607 Linearization Err: 5.1871325e-08\n",
      "Train loss: 0.027313069 Test loss: 0.19165108 Linearization Err: 5.1890886e-08\n",
      "Train loss: 0.026930338 Test loss: 0.1911067 Linearization Err: 5.1909673e-08\n",
      "Train loss: 0.026556537 Test loss: 0.19058213 Linearization Err: 5.1927792e-08\n",
      "Train loss: 0.02619125 Test loss: 0.1900763 Linearization Err: 5.1945342e-08\n",
      "Train loss: 0.025834123 Test loss: 0.18958838 Linearization Err: 5.1962417e-08\n",
      "Train loss: 0.025484808 Test loss: 0.18911766 Linearization Err: 5.1978986e-08\n",
      "Train loss: 0.025143003 Test loss: 0.18866332 Linearization Err: 5.199498e-08\n",
      "Train loss: 0.024808455 Test loss: 0.18822443 Linearization Err: 5.2010343e-08\n",
      "Train loss: 0.02448083 Test loss: 0.18780042 Linearization Err: 5.2025214e-08\n",
      "Train loss: 0.024159897 Test loss: 0.18739067 Linearization Err: 5.2039585e-08\n",
      "Train loss: 0.02384545 Test loss: 0.18699433 Linearization Err: 5.2053572e-08\n",
      "Train loss: 0.023537228 Test loss: 0.186611 Linearization Err: 5.206705e-08\n",
      "Train loss: 0.023235014 Test loss: 0.18624006 Linearization Err: 5.2080125e-08\n",
      "Train loss: 0.022938622 Test loss: 0.18588115 Linearization Err: 5.2092904e-08\n",
      "Train loss: 0.022647826 Test loss: 0.18553376 Linearization Err: 5.2105204e-08\n",
      "Train loss: 0.022362439 Test loss: 0.18519706 Linearization Err: 5.2116906e-08\n",
      "Train loss: 0.022082321 Test loss: 0.18487084 Linearization Err: 5.2128218e-08\n",
      "Train loss: 0.021807313 Test loss: 0.18455471 Linearization Err: 5.2139136e-08\n",
      "Train loss: 0.02153723 Test loss: 0.18424821 Linearization Err: 5.2149502e-08\n",
      "Train loss: 0.021271946 Test loss: 0.1839509 Linearization Err: 5.215953e-08\n",
      "Train loss: 0.02101129 Test loss: 0.18366249 Linearization Err: 5.216934e-08\n",
      "Train loss: 0.020755138 Test loss: 0.18338265 Linearization Err: 5.217887e-08\n",
      "Train loss: 0.020503387 Test loss: 0.18311088 Linearization Err: 5.2188e-08\n",
      "Train loss: 0.020255892 Test loss: 0.18284687 Linearization Err: 5.2196757e-08\n",
      "RelDiff 0.0016824073\n",
      "Diff 0.13865568\n",
      "Leak: 0.1\n",
      "Train loss: 5.026194 Test loss: 5.181761 Linearization Err: 0.0\n",
      "Train loss: 1.9443834 Test loss: 2.1658947 Linearization Err: 8.98362e-09\n",
      "Train loss: 1.0085297 Test loss: 1.241669 Linearization Err: 1.7899577e-08\n",
      "Train loss: 0.6836798 Test loss: 0.9211979 Linearization Err: 2.317618e-08\n",
      "Train loss: 0.5445212 Test loss: 0.7845342 Linearization Err: 2.6473586e-08\n",
      "Train loss: 0.46757454 Test loss: 0.70867956 Linearization Err: 2.8625273e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.41485298 Test loss: 0.6559087 Linearization Err: 3.0159896e-08\n",
      "Train loss: 0.37370437 Test loss: 0.6138608 Linearization Err: 3.1330195e-08\n",
      "Train loss: 0.33940762 Test loss: 0.57804286 Linearization Err: 3.2272396e-08\n",
      "Train loss: 0.30989808 Test loss: 0.54655135 Linearization Err: 3.3062157e-08\n",
      "Train loss: 0.28409377 Test loss: 0.5184186 Linearization Err: 3.374407e-08\n",
      "Train loss: 0.2613188 Test loss: 0.493052 Linearization Err: 3.434769e-08\n",
      "Train loss: 0.24109751 Test loss: 0.470042 Linearization Err: 3.4890363e-08\n",
      "Train loss: 0.22306828 Test loss: 0.44907975 Linearization Err: 3.538265e-08\n",
      "Train loss: 0.20693804 Test loss: 0.4299186 Linearization Err: 3.5833313e-08\n",
      "Train loss: 0.19246274 Test loss: 0.4123521 Linearization Err: 3.6249062e-08\n",
      "Train loss: 0.17943811 Test loss: 0.39620936 Linearization Err: 3.6631796e-08\n",
      "Train loss: 0.16768935 Test loss: 0.38134217 Linearization Err: 3.6986457e-08\n",
      "Train loss: 0.15706645 Test loss: 0.36762503 Linearization Err: 3.731685e-08\n",
      "Train loss: 0.14743963 Test loss: 0.3549462 Linearization Err: 3.7624368e-08\n",
      "Train loss: 0.13869737 Test loss: 0.34320801 Linearization Err: 3.7911015e-08\n",
      "Train loss: 0.13074253 Test loss: 0.33232427 Linearization Err: 3.817908e-08\n",
      "Train loss: 0.12348949 Test loss: 0.32221767 Linearization Err: 3.8429523e-08\n",
      "Train loss: 0.11686324 Test loss: 0.31281987 Linearization Err: 3.866471e-08\n",
      "Train loss: 0.11079821 Test loss: 0.30406952 Linearization Err: 3.888551e-08\n",
      "Train loss: 0.105236225 Test loss: 0.2959107 Linearization Err: 3.909245e-08\n",
      "Train loss: 0.100126594 Test loss: 0.28829393 Linearization Err: 3.928728e-08\n",
      "Train loss: 0.09542416 Test loss: 0.28117472 Linearization Err: 3.9471146e-08\n",
      "Train loss: 0.09108877 Test loss: 0.2745119 Linearization Err: 3.9644565e-08\n",
      "Train loss: 0.08708521 Test loss: 0.26826957 Linearization Err: 3.980854e-08\n",
      "Train loss: 0.083382204 Test loss: 0.26241514 Linearization Err: 3.9963403e-08\n",
      "Train loss: 0.07995096 Test loss: 0.25691828 Linearization Err: 4.0110137e-08\n",
      "Train loss: 0.0767666 Test loss: 0.25175205 Linearization Err: 4.0249343e-08\n",
      "Train loss: 0.073806785 Test loss: 0.24689117 Linearization Err: 4.0381074e-08\n",
      "Train loss: 0.0710514 Test loss: 0.2423132 Linearization Err: 4.0505988e-08\n",
      "Train loss: 0.06848262 Test loss: 0.2379983 Linearization Err: 4.0624798e-08\n",
      "Train loss: 0.06608404 Test loss: 0.2339271 Linearization Err: 4.073761e-08\n",
      "Train loss: 0.0638414 Test loss: 0.23008306 Linearization Err: 4.084508e-08\n",
      "Train loss: 0.061741702 Test loss: 0.22645073 Linearization Err: 4.0947377e-08\n",
      "Train loss: 0.059773125 Test loss: 0.22301511 Linearization Err: 4.1044657e-08\n",
      "Train loss: 0.05792485 Test loss: 0.21976307 Linearization Err: 4.113736e-08\n",
      "Train loss: 0.056187183 Test loss: 0.21668248 Linearization Err: 4.122549e-08\n",
      "Train loss: 0.054551467 Test loss: 0.21376222 Linearization Err: 4.1309352e-08\n",
      "Train loss: 0.053009745 Test loss: 0.21099192 Linearization Err: 4.138943e-08\n",
      "Train loss: 0.051554788 Test loss: 0.20836183 Linearization Err: 4.146596e-08\n",
      "Train loss: 0.050179973 Test loss: 0.20586301 Linearization Err: 4.153933e-08\n",
      "Train loss: 0.048879344 Test loss: 0.20348777 Linearization Err: 4.1609418e-08\n",
      "Train loss: 0.047647327 Test loss: 0.20122834 Linearization Err: 4.1676316e-08\n",
      "Train loss: 0.04647902 Test loss: 0.19907787 Linearization Err: 4.1740517e-08\n",
      "Train loss: 0.045369864 Test loss: 0.19702971 Linearization Err: 4.1801975e-08\n",
      "Train loss: 0.044315632 Test loss: 0.19507802 Linearization Err: 4.1860943e-08\n",
      "Train loss: 0.043312564 Test loss: 0.19321716 Linearization Err: 4.1917637e-08\n",
      "Train loss: 0.042357165 Test loss: 0.19144192 Linearization Err: 4.1972193e-08\n",
      "Train loss: 0.04144614 Test loss: 0.18974729 Linearization Err: 4.2024695e-08\n",
      "Train loss: 0.040576518 Test loss: 0.18812877 Linearization Err: 4.207505e-08\n",
      "Train loss: 0.039745547 Test loss: 0.18658201 Linearization Err: 4.212326e-08\n",
      "Train loss: 0.038950708 Test loss: 0.18510322 Linearization Err: 4.216971e-08\n",
      "Train loss: 0.03818963 Test loss: 0.18368864 Linearization Err: 4.2214197e-08\n",
      "Train loss: 0.037460137 Test loss: 0.18233469 Linearization Err: 4.2256886e-08\n",
      "Train loss: 0.036760323 Test loss: 0.18103828 Linearization Err: 4.2298e-08\n",
      "Train loss: 0.03608831 Test loss: 0.17979622 Linearization Err: 4.2337554e-08\n",
      "Train loss: 0.035442445 Test loss: 0.17860587 Linearization Err: 4.237569e-08\n",
      "Train loss: 0.034821197 Test loss: 0.17746456 Linearization Err: 4.2412548e-08\n",
      "Train loss: 0.034223046 Test loss: 0.17636964 Linearization Err: 4.244817e-08\n",
      "Train loss: 0.033646632 Test loss: 0.17531866 Linearization Err: 4.2482487e-08\n",
      "Train loss: 0.03309068 Test loss: 0.17430943 Linearization Err: 4.251572e-08\n",
      "Train loss: 0.032553982 Test loss: 0.17333971 Linearization Err: 4.2547935e-08\n",
      "Train loss: 0.032035537 Test loss: 0.17240785 Linearization Err: 4.2579245e-08\n",
      "Train loss: 0.031534307 Test loss: 0.1715119 Linearization Err: 4.2609486e-08\n",
      "Train loss: 0.03104934 Test loss: 0.17065 Linearization Err: 4.2638774e-08\n",
      "Train loss: 0.030579781 Test loss: 0.16982085 Linearization Err: 4.2666933e-08\n",
      "Train loss: 0.03012475 Test loss: 0.1690226 Linearization Err: 4.2694072e-08\n",
      "Train loss: 0.029683527 Test loss: 0.16825394 Linearization Err: 4.2720245e-08\n",
      "Train loss: 0.029255353 Test loss: 0.16751334 Linearization Err: 4.2745476e-08\n",
      "Train loss: 0.028839605 Test loss: 0.16679975 Linearization Err: 4.2769848e-08\n",
      "Train loss: 0.028435629 Test loss: 0.16611172 Linearization Err: 4.2793435e-08\n",
      "Train loss: 0.028042821 Test loss: 0.16544801 Linearization Err: 4.281609e-08\n",
      "Train loss: 0.027660644 Test loss: 0.16480772 Linearization Err: 4.2838145e-08\n",
      "Train loss: 0.027288586 Test loss: 0.16418983 Linearization Err: 4.285954e-08\n",
      "Train loss: 0.026926208 Test loss: 0.1635934 Linearization Err: 4.288019e-08\n",
      "Train loss: 0.02657307 Test loss: 0.16301732 Linearization Err: 4.2900243e-08\n",
      "Train loss: 0.026228761 Test loss: 0.16246085 Linearization Err: 4.2919712e-08\n",
      "Train loss: 0.02589284 Test loss: 0.16192275 Linearization Err: 4.2938694e-08\n",
      "Train loss: 0.025564972 Test loss: 0.16140251 Linearization Err: 4.2957055e-08\n",
      "Train loss: 0.025244787 Test loss: 0.16089924 Linearization Err: 4.297472e-08\n",
      "Train loss: 0.024932005 Test loss: 0.1604123 Linearization Err: 4.299182e-08\n",
      "Train loss: 0.024626296 Test loss: 0.15994118 Linearization Err: 4.300836e-08\n",
      "Train loss: 0.024327336 Test loss: 0.1594852 Linearization Err: 4.3024517e-08\n",
      "Train loss: 0.024034895 Test loss: 0.15904379 Linearization Err: 4.3040277e-08\n",
      "Train loss: 0.02374867 Test loss: 0.15861621 Linearization Err: 4.305557e-08\n",
      "Train loss: 0.023468435 Test loss: 0.15820205 Linearization Err: 4.30704e-08\n",
      "Train loss: 0.023193963 Test loss: 0.15780057 Linearization Err: 4.3084874e-08\n",
      "Train loss: 0.022924973 Test loss: 0.15741129 Linearization Err: 4.3099092e-08\n",
      "Train loss: 0.022661343 Test loss: 0.1570337 Linearization Err: 4.311297e-08\n",
      "Train loss: 0.022402843 Test loss: 0.15666737 Linearization Err: 4.3126438e-08\n",
      "Train loss: 0.022149287 Test loss: 0.15631193 Linearization Err: 4.313943e-08\n",
      "Train loss: 0.02190054 Test loss: 0.1559672 Linearization Err: 4.315202e-08\n",
      "Train loss: 0.021656418 Test loss: 0.15563238 Linearization Err: 4.3164277e-08\n",
      "Train loss: 0.02141673 Test loss: 0.15530732 Linearization Err: 4.3176055e-08\n",
      "Train loss: 0.021181323 Test loss: 0.1549915 Linearization Err: 4.3187363e-08\n",
      "RelDiff 0.0017509275\n",
      "Diff 0.12821233\n",
      "Leak: 0.25\n",
      "Train loss: 5.272697 Test loss: 5.464852 Linearization Err: 0.0\n",
      "Train loss: 1.5200161 Test loss: 1.6810958 Linearization Err: 6.747288e-09\n",
      "Train loss: 0.71441007 Test loss: 0.8679817 Linearization Err: 1.2444627e-08\n",
      "Train loss: 0.50610524 Test loss: 0.6626185 Linearization Err: 1.5513145e-08\n",
      "Train loss: 0.42951337 Test loss: 0.589431 Linearization Err: 1.7376772e-08\n",
      "Train loss: 0.38640004 Test loss: 0.54852 Linearization Err: 1.8658387e-08\n",
      "Train loss: 0.3543464 Test loss: 0.51775295 Linearization Err: 1.964279e-08\n",
      "Train loss: 0.3275594 Test loss: 0.4916965 Linearization Err: 2.0452665e-08\n",
      "Train loss: 0.3041801 Test loss: 0.46868458 Linearization Err: 2.1151711e-08\n",
      "Train loss: 0.28339365 Test loss: 0.4479907 Linearization Err: 2.1773836e-08\n",
      "Train loss: 0.26472804 Test loss: 0.4291808 Linearization Err: 2.2337543e-08\n",
      "Train loss: 0.247853 Test loss: 0.41195005 Linearization Err: 2.2854593e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.23251978 Test loss: 0.3960712 Linearization Err: 2.3332595e-08\n",
      "Train loss: 0.2185319 Test loss: 0.38136798 Linearization Err: 2.3777528e-08\n",
      "Train loss: 0.20572895 Test loss: 0.36770192 Linearization Err: 2.4192826e-08\n",
      "Train loss: 0.19397978 Test loss: 0.35496286 Linearization Err: 2.4581398e-08\n",
      "Train loss: 0.18317105 Test loss: 0.34305897 Linearization Err: 2.4947138e-08\n",
      "Train loss: 0.17320807 Test loss: 0.33191434 Linearization Err: 2.5291145e-08\n",
      "Train loss: 0.16400975 Test loss: 0.32146275 Linearization Err: 2.5615215e-08\n",
      "Train loss: 0.15550354 Test loss: 0.31164753 Linearization Err: 2.5921302e-08\n",
      "Train loss: 0.14762588 Test loss: 0.3024178 Linearization Err: 2.6208788e-08\n",
      "Train loss: 0.14032039 Test loss: 0.29372907 Linearization Err: 2.6477643e-08\n",
      "Train loss: 0.13353682 Test loss: 0.2855414 Linearization Err: 2.6731392e-08\n",
      "Train loss: 0.1272303 Test loss: 0.2778187 Linearization Err: 2.6971588e-08\n",
      "Train loss: 0.121361 Test loss: 0.2705278 Linearization Err: 2.719808e-08\n",
      "Train loss: 0.115892306 Test loss: 0.2636387 Linearization Err: 2.7411723e-08\n",
      "Train loss: 0.11079102 Test loss: 0.25712332 Linearization Err: 2.7613625e-08\n",
      "Train loss: 0.106027424 Test loss: 0.25095668 Linearization Err: 2.7804248e-08\n",
      "Train loss: 0.10157468 Test loss: 0.24511592 Linearization Err: 2.798424e-08\n",
      "Train loss: 0.097408116 Test loss: 0.2395791 Linearization Err: 2.8154286e-08\n",
      "Train loss: 0.093505226 Test loss: 0.23432621 Linearization Err: 2.8316261e-08\n",
      "Train loss: 0.08984591 Test loss: 0.22933953 Linearization Err: 2.8470357e-08\n",
      "Train loss: 0.08641201 Test loss: 0.22460224 Linearization Err: 2.8617045e-08\n",
      "Train loss: 0.083186716 Test loss: 0.22009891 Linearization Err: 2.8756606e-08\n",
      "Train loss: 0.08015444 Test loss: 0.21581541 Linearization Err: 2.8889787e-08\n",
      "Train loss: 0.077300884 Test loss: 0.21173796 Linearization Err: 2.9015924e-08\n",
      "Train loss: 0.074613094 Test loss: 0.20785421 Linearization Err: 2.9136292e-08\n",
      "Train loss: 0.072079554 Test loss: 0.20415306 Linearization Err: 2.9251243e-08\n",
      "Train loss: 0.06968915 Test loss: 0.20062307 Linearization Err: 2.9361086e-08\n",
      "Train loss: 0.067431934 Test loss: 0.19725455 Linearization Err: 2.9466396e-08\n",
      "Train loss: 0.06529869 Test loss: 0.1940386 Linearization Err: 2.956725e-08\n",
      "Train loss: 0.063280925 Test loss: 0.19096637 Linearization Err: 2.9663843e-08\n",
      "Train loss: 0.061370824 Test loss: 0.18802959 Linearization Err: 2.9756333e-08\n",
      "Train loss: 0.059561104 Test loss: 0.18522109 Linearization Err: 2.984471e-08\n",
      "Train loss: 0.057845134 Test loss: 0.18253365 Linearization Err: 2.992931e-08\n",
      "Train loss: 0.05621681 Test loss: 0.17996071 Linearization Err: 3.00103e-08\n",
      "Train loss: 0.054670475 Test loss: 0.17749605 Linearization Err: 3.008808e-08\n",
      "Train loss: 0.05320069 Test loss: 0.17513388 Linearization Err: 3.0162955e-08\n",
      "Train loss: 0.051802866 Test loss: 0.17286898 Linearization Err: 3.0234837e-08\n",
      "Train loss: 0.05047229 Test loss: 0.1706964 Linearization Err: 3.030376e-08\n",
      "Train loss: 0.049204897 Test loss: 0.16861159 Linearization Err: 3.0370337e-08\n",
      "Train loss: 0.04799683 Test loss: 0.16660996 Linearization Err: 3.0434613e-08\n",
      "Train loss: 0.04684451 Test loss: 0.16468729 Linearization Err: 3.0496587e-08\n",
      "Train loss: 0.04574449 Test loss: 0.16283967 Linearization Err: 3.0556425e-08\n",
      "Train loss: 0.044693768 Test loss: 0.1610633 Linearization Err: 3.0614085e-08\n",
      "Train loss: 0.04368948 Test loss: 0.15935498 Linearization Err: 3.0669554e-08\n",
      "Train loss: 0.042728953 Test loss: 0.15771142 Linearization Err: 3.0722862e-08\n",
      "Train loss: 0.04180967 Test loss: 0.1561297 Linearization Err: 3.0774302e-08\n",
      "Train loss: 0.040929284 Test loss: 0.15460691 Linearization Err: 3.0823777e-08\n",
      "Train loss: 0.040085554 Test loss: 0.15314011 Linearization Err: 3.0871394e-08\n",
      "Train loss: 0.039276484 Test loss: 0.15172681 Linearization Err: 3.0917295e-08\n",
      "Train loss: 0.038500145 Test loss: 0.15036476 Linearization Err: 3.096202e-08\n",
      "Train loss: 0.03775477 Test loss: 0.1490514 Linearization Err: 3.1005456e-08\n",
      "Train loss: 0.037038635 Test loss: 0.14778459 Linearization Err: 3.1047428e-08\n",
      "Train loss: 0.03635017 Test loss: 0.1465624 Linearization Err: 3.1087886e-08\n",
      "Train loss: 0.035687976 Test loss: 0.14538284 Linearization Err: 3.1126667e-08\n",
      "Train loss: 0.03505057 Test loss: 0.14424396 Linearization Err: 3.116422e-08\n",
      "Train loss: 0.034436718 Test loss: 0.14314407 Linearization Err: 3.1200607e-08\n",
      "Train loss: 0.033845287 Test loss: 0.14208154 Linearization Err: 3.123563e-08\n",
      "Train loss: 0.03327511 Test loss: 0.14105491 Linearization Err: 3.126942e-08\n",
      "Train loss: 0.03272515 Test loss: 0.14006257 Linearization Err: 3.1302136e-08\n",
      "Train loss: 0.03219428 Test loss: 0.13910282 Linearization Err: 3.1333826e-08\n",
      "Train loss: 0.031681627 Test loss: 0.13817467 Linearization Err: 3.136435e-08\n",
      "Train loss: 0.031186247 Test loss: 0.13727662 Linearization Err: 3.139401e-08\n",
      "Train loss: 0.030707354 Test loss: 0.13640763 Linearization Err: 3.1422722e-08\n",
      "Train loss: 0.030244127 Test loss: 0.13556635 Linearization Err: 3.1450547e-08\n",
      "Train loss: 0.02979587 Test loss: 0.13475189 Linearization Err: 3.1477743e-08\n",
      "Train loss: 0.029361801 Test loss: 0.133963 Linearization Err: 3.1504236e-08\n",
      "Train loss: 0.028941358 Test loss: 0.13319883 Linearization Err: 3.153016e-08\n",
      "Train loss: 0.02853383 Test loss: 0.13245822 Linearization Err: 3.1555473e-08\n",
      "Train loss: 0.028138712 Test loss: 0.13174048 Linearization Err: 3.158017e-08\n",
      "Train loss: 0.027755365 Test loss: 0.1310445 Linearization Err: 3.1604156e-08\n",
      "Train loss: 0.027383225 Test loss: 0.13036937 Linearization Err: 3.1627422e-08\n",
      "Train loss: 0.027021885 Test loss: 0.12971461 Linearization Err: 3.16502e-08\n",
      "Train loss: 0.0266708 Test loss: 0.12907927 Linearization Err: 3.1672336e-08\n",
      "Train loss: 0.026329568 Test loss: 0.12846276 Linearization Err: 3.169375e-08\n",
      "Train loss: 0.025997793 Test loss: 0.12786424 Linearization Err: 3.1714606e-08\n",
      "Train loss: 0.025675002 Test loss: 0.1272832 Linearization Err: 3.1735034e-08\n",
      "Train loss: 0.025360847 Test loss: 0.12671886 Linearization Err: 3.1754894e-08\n",
      "Train loss: 0.025054965 Test loss: 0.12617067 Linearization Err: 3.1774213e-08\n",
      "Train loss: 0.024756983 Test loss: 0.12563793 Linearization Err: 3.1793043e-08\n",
      "Train loss: 0.024466572 Test loss: 0.12512025 Linearization Err: 3.181125e-08\n",
      "Train loss: 0.024183454 Test loss: 0.12461698 Linearization Err: 3.1828982e-08\n",
      "Train loss: 0.023907334 Test loss: 0.12412759 Linearization Err: 3.1846156e-08\n",
      "Train loss: 0.023637906 Test loss: 0.123651676 Linearization Err: 3.186294e-08\n",
      "Train loss: 0.023374923 Test loss: 0.123188965 Linearization Err: 3.1879402e-08\n",
      "Train loss: 0.023118105 Test loss: 0.12273868 Linearization Err: 3.189543e-08\n",
      "Train loss: 0.022867221 Test loss: 0.12230063 Linearization Err: 3.191104e-08\n",
      "Train loss: 0.022622067 Test loss: 0.12187429 Linearization Err: 3.19262e-08\n",
      "Train loss: 0.02238239 Test loss: 0.12145927 Linearization Err: 3.1941077e-08\n",
      "RelDiff 0.0018021871\n",
      "Diff 0.11629481\n",
      "Leak: 0.5\n",
      "Train loss: 5.9505434 Test loss: 6.2562594 Linearization Err: 0.0\n",
      "Train loss: 0.8821755 Test loss: 0.9698663 Linearization Err: 2.219351e-09\n",
      "Train loss: 0.33412626 Test loss: 0.39870572 Linearization Err: 3.839948e-09\n",
      "Train loss: 0.25939965 Test loss: 0.32461438 Linearization Err: 4.6645185e-09\n",
      "Train loss: 0.2399955 Test loss: 0.30640277 Linearization Err: 5.255118e-09\n",
      "Train loss: 0.22830403 Test loss: 0.29539102 Linearization Err: 5.7565988e-09\n",
      "Train loss: 0.2183633 Test loss: 0.28599668 Linearization Err: 6.2126944e-09\n",
      "Train loss: 0.20930098 Test loss: 0.27748048 Linearization Err: 6.6392483e-09\n",
      "Train loss: 0.20092133 Test loss: 0.26965284 Linearization Err: 7.0427992e-09\n",
      "Train loss: 0.19313282 Test loss: 0.26240277 Linearization Err: 7.4270794e-09\n",
      "Train loss: 0.18586797 Test loss: 0.25564668 Linearization Err: 7.794008e-09\n",
      "Train loss: 0.17907105 Test loss: 0.249319 Linearization Err: 8.143843e-09\n",
      "Train loss: 0.17269433 Test loss: 0.24336746 Linearization Err: 8.478598e-09\n",
      "Train loss: 0.16669606 Test loss: 0.23774962 Linearization Err: 8.799415e-09\n",
      "Train loss: 0.16104038 Test loss: 0.2324304 Linearization Err: 9.106694e-09\n",
      "Train loss: 0.15569605 Test loss: 0.22737989 Linearization Err: 9.401627e-09\n",
      "Train loss: 0.15063548 Test loss: 0.22257273 Linearization Err: 9.685378e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1458347 Test loss: 0.21798748 Linearization Err: 9.957581e-09\n",
      "Train loss: 0.14127256 Test loss: 0.21360528 Linearization Err: 1.021985e-08\n",
      "Train loss: 0.13693045 Test loss: 0.20941016 Linearization Err: 1.0472954e-08\n",
      "Train loss: 0.13279203 Test loss: 0.20538752 Linearization Err: 1.07170255e-08\n",
      "Train loss: 0.12884292 Test loss: 0.20152472 Linearization Err: 1.0952444e-08\n",
      "Train loss: 0.12506975 Test loss: 0.19781083 Linearization Err: 1.117981e-08\n",
      "Train loss: 0.121461004 Test loss: 0.19423579 Linearization Err: 1.1400029e-08\n",
      "Train loss: 0.118006155 Test loss: 0.19079116 Linearization Err: 1.1612906e-08\n",
      "Train loss: 0.11469568 Test loss: 0.187469 Linearization Err: 1.1818689e-08\n",
      "Train loss: 0.11152086 Test loss: 0.1842618 Linearization Err: 1.20179715e-08\n",
      "Train loss: 0.10847393 Test loss: 0.18116343 Linearization Err: 1.2211119e-08\n",
      "Train loss: 0.10554771 Test loss: 0.17816809 Linearization Err: 1.239855e-08\n",
      "Train loss: 0.10273554 Test loss: 0.1752705 Linearization Err: 1.2580555e-08\n",
      "Train loss: 0.10003153 Test loss: 0.17246585 Linearization Err: 1.2757226e-08\n",
      "Train loss: 0.09743015 Test loss: 0.16974978 Linearization Err: 1.2929046e-08\n",
      "Train loss: 0.094926216 Test loss: 0.16711839 Linearization Err: 1.3096319e-08\n",
      "Train loss: 0.09251502 Test loss: 0.16456771 Linearization Err: 1.3258658e-08\n",
      "Train loss: 0.09019216 Test loss: 0.16209422 Linearization Err: 1.3415737e-08\n",
      "Train loss: 0.08795345 Test loss: 0.15969487 Linearization Err: 1.3567652e-08\n",
      "Train loss: 0.0857949 Test loss: 0.15736617 Linearization Err: 1.3715631e-08\n",
      "Train loss: 0.08371306 Test loss: 0.15510552 Linearization Err: 1.385934e-08\n",
      "Train loss: 0.08170443 Test loss: 0.15291014 Linearization Err: 1.39983065e-08\n",
      "Train loss: 0.07976575 Test loss: 0.15077777 Linearization Err: 1.4132369e-08\n",
      "Train loss: 0.07789408 Test loss: 0.14870594 Linearization Err: 1.4261712e-08\n",
      "Train loss: 0.07608645 Test loss: 0.14669213 Linearization Err: 1.4386643e-08\n",
      "Train loss: 0.07434019 Test loss: 0.14473441 Linearization Err: 1.4507815e-08\n",
      "Train loss: 0.07265271 Test loss: 0.14283085 Linearization Err: 1.4625712e-08\n",
      "Train loss: 0.07102173 Test loss: 0.14097942 Linearization Err: 1.474055e-08\n",
      "Train loss: 0.06944477 Test loss: 0.13917829 Linearization Err: 1.485196e-08\n",
      "Train loss: 0.06791974 Test loss: 0.13742548 Linearization Err: 1.495929e-08\n",
      "Train loss: 0.066444494 Test loss: 0.13571964 Linearization Err: 1.506221e-08\n",
      "Train loss: 0.06501699 Test loss: 0.13405885 Linearization Err: 1.5161671e-08\n",
      "Train loss: 0.06363541 Test loss: 0.13244182 Linearization Err: 1.525778e-08\n",
      "Train loss: 0.062297974 Test loss: 0.13086726 Linearization Err: 1.5350327e-08\n",
      "Train loss: 0.061002985 Test loss: 0.1293335 Linearization Err: 1.5439353e-08\n",
      "Train loss: 0.059748806 Test loss: 0.12783933 Linearization Err: 1.5525654e-08\n",
      "Train loss: 0.058533803 Test loss: 0.12638336 Linearization Err: 1.5609109e-08\n",
      "Train loss: 0.057356562 Test loss: 0.12496438 Linearization Err: 1.5689702e-08\n",
      "Train loss: 0.056215674 Test loss: 0.1235812 Linearization Err: 1.5767675e-08\n",
      "Train loss: 0.05510986 Test loss: 0.12223279 Linearization Err: 1.5843304e-08\n",
      "Train loss: 0.054037716 Test loss: 0.12091806 Linearization Err: 1.5916624e-08\n",
      "Train loss: 0.05299795 Test loss: 0.11963593 Linearization Err: 1.5988032e-08\n",
      "Train loss: 0.05198943 Test loss: 0.11838528 Linearization Err: 1.6057228e-08\n",
      "Train loss: 0.051011004 Test loss: 0.11716521 Linearization Err: 1.6124334e-08\n",
      "Train loss: 0.05006158 Test loss: 0.11597471 Linearization Err: 1.6189432e-08\n",
      "Train loss: 0.049140017 Test loss: 0.11481306 Linearization Err: 1.6252725e-08\n",
      "Train loss: 0.04824544 Test loss: 0.11367908 Linearization Err: 1.631423e-08\n",
      "Train loss: 0.047376804 Test loss: 0.11257226 Linearization Err: 1.6373434e-08\n",
      "Train loss: 0.046533283 Test loss: 0.11149161 Linearization Err: 1.6430896e-08\n",
      "Train loss: 0.045713905 Test loss: 0.11043639 Linearization Err: 1.6486556e-08\n",
      "Train loss: 0.04491784 Test loss: 0.10940576 Linearization Err: 1.6540621e-08\n",
      "Train loss: 0.044144243 Test loss: 0.10839917 Linearization Err: 1.6593315e-08\n",
      "Train loss: 0.043392386 Test loss: 0.107415944 Linearization Err: 1.6644396e-08\n",
      "Train loss: 0.04266151 Test loss: 0.10645518 Linearization Err: 1.6693964e-08\n",
      "Train loss: 0.041950867 Test loss: 0.10551633 Linearization Err: 1.6741867e-08\n",
      "Train loss: 0.041259736 Test loss: 0.104598925 Linearization Err: 1.678816e-08\n",
      "Train loss: 0.040587533 Test loss: 0.103702106 Linearization Err: 1.6833274e-08\n",
      "Train loss: 0.03993355 Test loss: 0.102825336 Linearization Err: 1.6877216e-08\n",
      "Train loss: 0.03929722 Test loss: 0.101968 Linearization Err: 1.6920035e-08\n",
      "Train loss: 0.038677987 Test loss: 0.101129666 Linearization Err: 1.696186e-08\n",
      "Train loss: 0.038075235 Test loss: 0.10030992 Linearization Err: 1.7002463e-08\n",
      "Train loss: 0.037488345 Test loss: 0.099508055 Linearization Err: 1.7041849e-08\n",
      "Train loss: 0.036916878 Test loss: 0.09872359 Linearization Err: 1.7080215e-08\n",
      "Train loss: 0.03636033 Test loss: 0.09795606 Linearization Err: 1.711782e-08\n",
      "Train loss: 0.035818245 Test loss: 0.09720514 Linearization Err: 1.7154651e-08\n",
      "Train loss: 0.03529009 Test loss: 0.09647021 Linearization Err: 1.7190551e-08\n",
      "Train loss: 0.03477549 Test loss: 0.09575086 Linearization Err: 1.7225435e-08\n",
      "Train loss: 0.03427396 Test loss: 0.09504676 Linearization Err: 1.7259442e-08\n",
      "Train loss: 0.03378507 Test loss: 0.09435743 Linearization Err: 1.7292875e-08\n",
      "Train loss: 0.03330844 Test loss: 0.09368242 Linearization Err: 1.7325197e-08\n",
      "Train loss: 0.03284367 Test loss: 0.093021594 Linearization Err: 1.7356534e-08\n",
      "Train loss: 0.032390434 Test loss: 0.0923744 Linearization Err: 1.73873e-08\n",
      "Train loss: 0.031948347 Test loss: 0.09174061 Linearization Err: 1.7417431e-08\n",
      "Train loss: 0.031517062 Test loss: 0.09111979 Linearization Err: 1.7446881e-08\n",
      "Train loss: 0.031096213 Test loss: 0.090511434 Linearization Err: 1.747576e-08\n",
      "Train loss: 0.03068551 Test loss: 0.08991537 Linearization Err: 1.7503947e-08\n",
      "Train loss: 0.030284628 Test loss: 0.08933142 Linearization Err: 1.7531463e-08\n",
      "Train loss: 0.029893264 Test loss: 0.088759035 Linearization Err: 1.7558428e-08\n",
      "Train loss: 0.029511118 Test loss: 0.08819804 Linearization Err: 1.758465e-08\n",
      "Train loss: 0.029137943 Test loss: 0.08764809 Linearization Err: 1.7610144e-08\n",
      "Train loss: 0.028773455 Test loss: 0.087108985 Linearization Err: 1.7635188e-08\n",
      "Train loss: 0.02841737 Test loss: 0.08658042 Linearization Err: 1.765963e-08\n",
      "Train loss: 0.028069496 Test loss: 0.08606205 Linearization Err: 1.7683416e-08\n",
      "RelDiff 0.0016526869\n",
      "Diff 0.10064758\n",
      "Leak: 0.75\n",
      "Train loss: 6.9621496 Test loss: 7.447153 Linearization Err: 0.0\n",
      "Train loss: 0.4143207 Test loss: 0.459389 Linearization Err: 5.8806005e-10\n",
      "Train loss: 0.09743395 Test loss: 0.11475919 Linearization Err: 7.611165e-10\n",
      "Train loss: 0.078653276 Test loss: 0.09437182 Linearization Err: 8.560652e-10\n",
      "Train loss: 0.07656254 Test loss: 0.091925606 Linearization Err: 9.2844415e-10\n",
      "Train loss: 0.075555146 Test loss: 0.09078175 Linearization Err: 9.950312e-10\n",
      "Train loss: 0.07464666 Test loss: 0.08985802 Linearization Err: 1.059748e-09\n",
      "Train loss: 0.0737656 Test loss: 0.08902137 Linearization Err: 1.123103e-09\n",
      "Train loss: 0.0729054 Test loss: 0.088231355 Linearization Err: 1.1853818e-09\n",
      "Train loss: 0.07206471 Test loss: 0.08747072 Linearization Err: 1.2470125e-09\n",
      "Train loss: 0.07124286 Test loss: 0.08673197 Linearization Err: 1.3082045e-09\n",
      "Train loss: 0.070439205 Test loss: 0.08601195 Linearization Err: 1.3688946e-09\n",
      "Train loss: 0.06965308 Test loss: 0.08530863 Linearization Err: 1.4290145e-09\n",
      "Train loss: 0.068883926 Test loss: 0.084621176 Linearization Err: 1.4883903e-09\n",
      "Train loss: 0.068131275 Test loss: 0.08394895 Linearization Err: 1.5467824e-09\n",
      "Train loss: 0.06739453 Test loss: 0.08329122 Linearization Err: 1.6045635e-09\n",
      "Train loss: 0.06667317 Test loss: 0.0826476 Linearization Err: 1.661661e-09\n",
      "Train loss: 0.06596671 Test loss: 0.082017556 Linearization Err: 1.7181164e-09\n",
      "Train loss: 0.06527469 Test loss: 0.08140058 Linearization Err: 1.7738888e-09\n",
      "Train loss: 0.06459672 Test loss: 0.08079636 Linearization Err: 1.8289781e-09\n",
      "Train loss: 0.063932255 Test loss: 0.08020434 Linearization Err: 1.8836976e-09\n",
      "Train loss: 0.063280895 Test loss: 0.07962422 Linearization Err: 1.9376556e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.06264225 Test loss: 0.07905552 Linearization Err: 1.9910598e-09\n",
      "Train loss: 0.06201596 Test loss: 0.07849797 Linearization Err: 2.0439659e-09\n",
      "Train loss: 0.061401647 Test loss: 0.07795129 Linearization Err: 2.0960644e-09\n",
      "Train loss: 0.06079894 Test loss: 0.07741488 Linearization Err: 2.1475641e-09\n",
      "Train loss: 0.060207468 Test loss: 0.07688842 Linearization Err: 2.198316e-09\n",
      "Train loss: 0.05962687 Test loss: 0.07637181 Linearization Err: 2.2484112e-09\n",
      "Train loss: 0.059056927 Test loss: 0.07586475 Linearization Err: 2.2979294e-09\n",
      "Train loss: 0.058497194 Test loss: 0.075366855 Linearization Err: 2.3468618e-09\n",
      "Train loss: 0.057947468 Test loss: 0.07487769 Linearization Err: 2.3952573e-09\n",
      "Train loss: 0.057407405 Test loss: 0.07439722 Linearization Err: 2.4432902e-09\n",
      "Train loss: 0.056876812 Test loss: 0.07392501 Linearization Err: 2.4907705e-09\n",
      "Train loss: 0.056355316 Test loss: 0.07346103 Linearization Err: 2.5381692e-09\n",
      "Train loss: 0.055842765 Test loss: 0.07300481 Linearization Err: 2.5853768e-09\n",
      "Train loss: 0.055338856 Test loss: 0.072556324 Linearization Err: 2.6320033e-09\n",
      "Train loss: 0.054843318 Test loss: 0.07211519 Linearization Err: 2.678003e-09\n",
      "Train loss: 0.054356046 Test loss: 0.07168127 Linearization Err: 2.723254e-09\n",
      "Train loss: 0.053876672 Test loss: 0.07125429 Linearization Err: 2.7699292e-09\n",
      "Train loss: 0.05340506 Test loss: 0.07083411 Linearization Err: 2.822575e-09\n",
      "Train loss: 0.052940983 Test loss: 0.07042049 Linearization Err: 2.8744718e-09\n",
      "Train loss: 0.052484248 Test loss: 0.07001327 Linearization Err: 2.925209e-09\n",
      "Train loss: 0.052034654 Test loss: 0.06961234 Linearization Err: 2.9745597e-09\n",
      "Train loss: 0.051592056 Test loss: 0.06921744 Linearization Err: 3.022556e-09\n",
      "Train loss: 0.051156223 Test loss: 0.06882841 Linearization Err: 3.0690899e-09\n",
      "Train loss: 0.05072702 Test loss: 0.068445176 Linearization Err: 3.1136067e-09\n",
      "Train loss: 0.050304286 Test loss: 0.068067536 Linearization Err: 3.1562568e-09\n",
      "Train loss: 0.049887843 Test loss: 0.06769535 Linearization Err: 3.1972966e-09\n",
      "Train loss: 0.04947748 Test loss: 0.06732842 Linearization Err: 3.236936e-09\n",
      "Train loss: 0.049073108 Test loss: 0.06696657 Linearization Err: 3.2751133e-09\n",
      "Train loss: 0.04867461 Test loss: 0.06660979 Linearization Err: 3.311631e-09\n",
      "Train loss: 0.04828182 Test loss: 0.066257864 Linearization Err: 3.346855e-09\n",
      "Train loss: 0.047894556 Test loss: 0.06591079 Linearization Err: 3.3808831e-09\n",
      "Train loss: 0.047512762 Test loss: 0.06556833 Linearization Err: 3.4135978e-09\n",
      "Train loss: 0.04713628 Test loss: 0.06523042 Linearization Err: 3.4449497e-09\n",
      "Train loss: 0.046764977 Test loss: 0.06489695 Linearization Err: 3.474852e-09\n",
      "Train loss: 0.04639873 Test loss: 0.06456792 Linearization Err: 3.5037189e-09\n",
      "Train loss: 0.04603749 Test loss: 0.06424305 Linearization Err: 3.5316274e-09\n",
      "Train loss: 0.045681078 Test loss: 0.06392231 Linearization Err: 3.5670924e-09\n",
      "Train loss: 0.0453294 Test loss: 0.06360558 Linearization Err: 3.6028098e-09\n",
      "Train loss: 0.044982377 Test loss: 0.063292794 Linearization Err: 3.654006e-09\n",
      "Train loss: 0.044639885 Test loss: 0.06298384 Linearization Err: 3.7059555e-09\n",
      "Train loss: 0.044301808 Test loss: 0.06267858 Linearization Err: 3.7573553e-09\n",
      "Train loss: 0.04396811 Test loss: 0.06237704 Linearization Err: 3.808486e-09\n",
      "Train loss: 0.043638665 Test loss: 0.06207904 Linearization Err: 3.859298e-09\n",
      "Train loss: 0.043313384 Test loss: 0.06178463 Linearization Err: 3.90953e-09\n",
      "Train loss: 0.04299217 Test loss: 0.061493687 Linearization Err: 3.95885e-09\n",
      "Train loss: 0.042674996 Test loss: 0.061206043 Linearization Err: 4.0074646e-09\n",
      "Train loss: 0.042361744 Test loss: 0.060921818 Linearization Err: 4.055746e-09\n",
      "Train loss: 0.042052332 Test loss: 0.06064076 Linearization Err: 4.1033825e-09\n",
      "Train loss: 0.041746695 Test loss: 0.060362827 Linearization Err: 4.150539e-09\n",
      "Train loss: 0.041444734 Test loss: 0.060088012 Linearization Err: 4.197412e-09\n",
      "Train loss: 0.041146424 Test loss: 0.059816156 Linearization Err: 4.243906e-09\n",
      "Train loss: 0.040851634 Test loss: 0.059547268 Linearization Err: 4.2900776e-09\n",
      "Train loss: 0.040560357 Test loss: 0.059281327 Linearization Err: 4.3357655e-09\n",
      "Train loss: 0.040272456 Test loss: 0.05901823 Linearization Err: 4.3810005e-09\n",
      "Train loss: 0.039987955 Test loss: 0.058757972 Linearization Err: 4.42596e-09\n",
      "Train loss: 0.03970676 Test loss: 0.05850043 Linearization Err: 4.4705817e-09\n",
      "Train loss: 0.0394288 Test loss: 0.058245596 Linearization Err: 4.514797e-09\n",
      "Train loss: 0.03915399 Test loss: 0.05799346 Linearization Err: 4.5584096e-09\n",
      "Train loss: 0.0388823 Test loss: 0.057743926 Linearization Err: 4.6016364e-09\n",
      "Train loss: 0.038613684 Test loss: 0.057496972 Linearization Err: 4.644293e-09\n",
      "Train loss: 0.038348086 Test loss: 0.057252537 Linearization Err: 4.686654e-09\n",
      "Train loss: 0.038085483 Test loss: 0.0570105 Linearization Err: 4.7286783e-09\n",
      "Train loss: 0.037825767 Test loss: 0.056770932 Linearization Err: 4.7702793e-09\n",
      "Train loss: 0.037568957 Test loss: 0.056533724 Linearization Err: 4.8116107e-09\n",
      "Train loss: 0.037314944 Test loss: 0.05629893 Linearization Err: 4.8524553e-09\n",
      "Train loss: 0.037063684 Test loss: 0.05606642 Linearization Err: 4.892953e-09\n",
      "Train loss: 0.036815178 Test loss: 0.055836126 Linearization Err: 4.93314e-09\n",
      "Train loss: 0.03656931 Test loss: 0.055608056 Linearization Err: 4.9730913e-09\n",
      "Train loss: 0.036326103 Test loss: 0.055382196 Linearization Err: 5.0126503e-09\n",
      "Train loss: 0.036085483 Test loss: 0.055158563 Linearization Err: 5.0518327e-09\n",
      "Train loss: 0.035847425 Test loss: 0.054936957 Linearization Err: 5.0908646e-09\n",
      "Train loss: 0.035611894 Test loss: 0.05471752 Linearization Err: 5.1296905e-09\n",
      "Train loss: 0.035378825 Test loss: 0.054500155 Linearization Err: 5.168174e-09\n",
      "Train loss: 0.03514822 Test loss: 0.054284763 Linearization Err: 5.2064832e-09\n",
      "Train loss: 0.034919985 Test loss: 0.054071367 Linearization Err: 5.2443543e-09\n",
      "Train loss: 0.034694124 Test loss: 0.053859927 Linearization Err: 5.2819393e-09\n",
      "Train loss: 0.034470573 Test loss: 0.053650424 Linearization Err: 5.3193445e-09\n",
      "Train loss: 0.034249336 Test loss: 0.05344288 Linearization Err: 5.3563123e-09\n",
      "RelDiff 0.001117345\n",
      "Diff 0.081628405\n",
      "Leak: 1\n",
      "Train loss: 8.307516 Test loss: 9.037533 Linearization Err: 0.0\n",
      "Train loss: 0.16761914 Test loss: 0.2000927 Linearization Err: 9.2236535e-10\n",
      "Train loss: 0.011248805 Test loss: 0.016342163 Linearization Err: 1.0266384e-09\n",
      "Train loss: 0.007388931 Test loss: 0.010423507 Linearization Err: 1.0478697e-09\n",
      "Train loss: 0.007267605 Test loss: 0.00996341 Linearization Err: 1.0516876e-09\n",
      "Train loss: 0.007262425 Test loss: 0.0098848045 Linearization Err: 1.0523689e-09\n",
      "Train loss: 0.0072621177 Test loss: 0.009868044 Linearization Err: 1.0524454e-09\n",
      "Train loss: 0.0072620856 Test loss: 0.009863889 Linearization Err: 1.052452e-09\n",
      "Train loss: 0.0072620856 Test loss: 0.009862324 Linearization Err: 1.0524644e-09\n",
      "Train loss: 0.007262086 Test loss: 0.009861534 Linearization Err: 1.0524698e-09\n",
      "Train loss: 0.0072620814 Test loss: 0.009861057 Linearization Err: 1.0524704e-09\n",
      "Train loss: 0.0072620804 Test loss: 0.009860736 Linearization Err: 1.0524694e-09\n",
      "Train loss: 0.007262072 Test loss: 0.009860501 Linearization Err: 1.0524701e-09\n",
      "Train loss: 0.00726208 Test loss: 0.009860337 Linearization Err: 1.0524713e-09\n",
      "Train loss: 0.007262079 Test loss: 0.009860193 Linearization Err: 1.0524728e-09\n",
      "Train loss: 0.007262082 Test loss: 0.009860092 Linearization Err: 1.0524733e-09\n",
      "Train loss: 0.0072620786 Test loss: 0.009859999 Linearization Err: 1.0524737e-09\n",
      "Train loss: 0.0072620795 Test loss: 0.009859924 Linearization Err: 1.0524743e-09\n",
      "Train loss: 0.007262075 Test loss: 0.009859856 Linearization Err: 1.0524752e-09\n",
      "Train loss: 0.0072620767 Test loss: 0.00985981 Linearization Err: 1.0524758e-09\n",
      "Train loss: 0.0072620804 Test loss: 0.009859765 Linearization Err: 1.0524761e-09\n",
      "Train loss: 0.0072620716 Test loss: 0.00985972 Linearization Err: 1.0524759e-09\n",
      "Train loss: 0.0072620753 Test loss: 0.009859689 Linearization Err: 1.0524761e-09\n",
      "Train loss: 0.0072620763 Test loss: 0.009859659 Linearization Err: 1.0524759e-09\n",
      "Train loss: 0.0072620786 Test loss: 0.009859625 Linearization Err: 1.0524756e-09\n",
      "Train loss: 0.0072620753 Test loss: 0.009859601 Linearization Err: 1.0524751e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.007262076 Test loss: 0.009859583 Linearization Err: 1.0524748e-09\n",
      "Train loss: 0.007262073 Test loss: 0.009859563 Linearization Err: 1.0524739e-09\n",
      "Train loss: 0.0072620735 Test loss: 0.0098595405 Linearization Err: 1.0524743e-09\n",
      "Train loss: 0.0072620735 Test loss: 0.009859526 Linearization Err: 1.0524738e-09\n",
      "Train loss: 0.007262072 Test loss: 0.009859513 Linearization Err: 1.0524736e-09\n",
      "Train loss: 0.007262072 Test loss: 0.009859496 Linearization Err: 1.0524734e-09\n",
      "Train loss: 0.007262075 Test loss: 0.009859482 Linearization Err: 1.052473e-09\n",
      "Train loss: 0.0072620735 Test loss: 0.00985947 Linearization Err: 1.0524733e-09\n",
      "Train loss: 0.007262075 Test loss: 0.009859458 Linearization Err: 1.0524732e-09\n",
      "Train loss: 0.007262075 Test loss: 0.009859439 Linearization Err: 1.0524733e-09\n",
      "Train loss: 0.0072620786 Test loss: 0.009859433 Linearization Err: 1.0524729e-09\n",
      "Train loss: 0.0072620795 Test loss: 0.009859427 Linearization Err: 1.0524729e-09\n",
      "Train loss: 0.007262076 Test loss: 0.009859421 Linearization Err: 1.0524728e-09\n",
      "Train loss: 0.007262071 Test loss: 0.009859405 Linearization Err: 1.0524726e-09\n",
      "Train loss: 0.007262071 Test loss: 0.009859392 Linearization Err: 1.0524724e-09\n",
      "Train loss: 0.007262078 Test loss: 0.009859389 Linearization Err: 1.0524726e-09\n",
      "Train loss: 0.0072620786 Test loss: 0.009859381 Linearization Err: 1.0524723e-09\n",
      "Train loss: 0.0072620767 Test loss: 0.009859374 Linearization Err: 1.0524726e-09\n",
      "Train loss: 0.0072620767 Test loss: 0.009859367 Linearization Err: 1.0524724e-09\n",
      "Train loss: 0.0072620795 Test loss: 0.00985936 Linearization Err: 1.0524724e-09\n",
      "Train loss: 0.0072620767 Test loss: 0.009859352 Linearization Err: 1.052473e-09\n",
      "Train loss: 0.007262076 Test loss: 0.009859345 Linearization Err: 1.0524728e-09\n",
      "Train loss: 0.007262072 Test loss: 0.009859344 Linearization Err: 1.0524723e-09\n",
      "Train loss: 0.0072620674 Test loss: 0.009859344 Linearization Err: 1.0524726e-09\n",
      "Train loss: 0.007262071 Test loss: 0.009859335 Linearization Err: 1.0524724e-09\n",
      "Train loss: 0.007262074 Test loss: 0.009859332 Linearization Err: 1.0524722e-09\n",
      "Train loss: 0.007262073 Test loss: 0.009859327 Linearization Err: 1.0524724e-09\n",
      "Train loss: 0.00726207 Test loss: 0.009859324 Linearization Err: 1.0524723e-09\n",
      "Train loss: 0.0072620693 Test loss: 0.00985932 Linearization Err: 1.052472e-09\n",
      "Train loss: 0.007262072 Test loss: 0.009859313 Linearization Err: 1.0524721e-09\n",
      "Train loss: 0.0072620744 Test loss: 0.00985931 Linearization Err: 1.052472e-09\n",
      "Train loss: 0.007262072 Test loss: 0.009859305 Linearization Err: 1.0524721e-09\n",
      "Train loss: 0.007262073 Test loss: 0.009859297 Linearization Err: 1.0524721e-09\n",
      "Train loss: 0.007262075 Test loss: 0.0098592965 Linearization Err: 1.052472e-09\n",
      "Train loss: 0.007262073 Test loss: 0.009859296 Linearization Err: 1.0524719e-09\n",
      "Train loss: 0.007262072 Test loss: 0.00985929 Linearization Err: 1.0524719e-09\n",
      "Train loss: 0.007262074 Test loss: 0.009859288 Linearization Err: 1.0524719e-09\n",
      "Train loss: 0.0072620767 Test loss: 0.009859282 Linearization Err: 1.0524721e-09\n",
      "Train loss: 0.0072620744 Test loss: 0.009859283 Linearization Err: 1.052472e-09\n",
      "Train loss: 0.0072620725 Test loss: 0.009859282 Linearization Err: 1.0524721e-09\n",
      "Train loss: 0.0072620763 Test loss: 0.009859278 Linearization Err: 1.052472e-09\n",
      "Train loss: 0.007262073 Test loss: 0.009859275 Linearization Err: 1.0524721e-09\n",
      "Train loss: 0.007262074 Test loss: 0.009859272 Linearization Err: 1.052472e-09\n",
      "Train loss: 0.0072620725 Test loss: 0.00985927 Linearization Err: 1.0524719e-09\n",
      "Train loss: 0.007262072 Test loss: 0.009859269 Linearization Err: 1.052472e-09\n",
      "Train loss: 0.00726207 Test loss: 0.009859265 Linearization Err: 1.0524719e-09\n",
      "Train loss: 0.0072620707 Test loss: 0.009859261 Linearization Err: 1.0524718e-09\n",
      "Train loss: 0.0072620697 Test loss: 0.009859259 Linearization Err: 1.0524718e-09\n",
      "Train loss: 0.007262069 Test loss: 0.00985926 Linearization Err: 1.0524719e-09\n",
      "Train loss: 0.0072620693 Test loss: 0.009859259 Linearization Err: 1.0524721e-09\n",
      "Train loss: 0.0072620683 Test loss: 0.009859257 Linearization Err: 1.0524718e-09\n",
      "Train loss: 0.0072620707 Test loss: 0.009859254 Linearization Err: 1.0524718e-09\n",
      "Train loss: 0.00726207 Test loss: 0.009859254 Linearization Err: 1.0524718e-09\n",
      "Train loss: 0.0072620674 Test loss: 0.009859251 Linearization Err: 1.0524719e-09\n",
      "Train loss: 0.0072620693 Test loss: 0.009859249 Linearization Err: 1.0524723e-09\n",
      "Train loss: 0.00726207 Test loss: 0.009859243 Linearization Err: 1.0524723e-09\n",
      "Train loss: 0.0072620716 Test loss: 0.0098592425 Linearization Err: 1.0524723e-09\n",
      "Train loss: 0.007262073 Test loss: 0.0098592425 Linearization Err: 1.0524723e-09\n",
      "Train loss: 0.007262073 Test loss: 0.0098592425 Linearization Err: 1.0524721e-09\n",
      "Train loss: 0.0072620716 Test loss: 0.009859242 Linearization Err: 1.0524721e-09\n",
      "Train loss: 0.007262072 Test loss: 0.009859243 Linearization Err: 1.0524721e-09\n",
      "Train loss: 0.007262072 Test loss: 0.009859242 Linearization Err: 1.0524724e-09\n",
      "Train loss: 0.007262069 Test loss: 0.009859242 Linearization Err: 1.0524722e-09\n",
      "Train loss: 0.00726207 Test loss: 0.009859237 Linearization Err: 1.0524723e-09\n",
      "Train loss: 0.007262071 Test loss: 0.009859239 Linearization Err: 1.0524723e-09\n",
      "Train loss: 0.0072620707 Test loss: 0.009859239 Linearization Err: 1.0524722e-09\n",
      "Train loss: 0.007262072 Test loss: 0.009859234 Linearization Err: 1.0524721e-09\n",
      "Train loss: 0.007262072 Test loss: 0.009859232 Linearization Err: 1.0524722e-09\n",
      "Train loss: 0.007262071 Test loss: 0.00985923 Linearization Err: 1.0524721e-09\n",
      "Train loss: 0.0072620716 Test loss: 0.009859231 Linearization Err: 1.0524721e-09\n",
      "Train loss: 0.007262072 Test loss: 0.009859226 Linearization Err: 1.0524721e-09\n",
      "Train loss: 0.0072620725 Test loss: 0.009859227 Linearization Err: 1.0524722e-09\n",
      "Train loss: 0.0072620735 Test loss: 0.009859225 Linearization Err: 1.0524724e-09\n",
      "Train loss: 0.007262073 Test loss: 0.009859226 Linearization Err: 1.0524722e-09\n",
      "RelDiff 0.09291845\n",
      "Diff 0.8537657\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.000001\n",
    "\n",
    "\n",
    "for i in range(len(l_vec)):\n",
    "    leak = l_vec[i]\n",
    "    print('Leak:', leak)\n",
    "    W = W_init\n",
    "    v = v_init\n",
    "    eig_vecs_priors = jnp.zeros(shape = (3, 15, dim) )\n",
    "    loss_cl = lambda x: loss(x, data_train, leak)\n",
    "\n",
    "    for t in range(T):\n",
    "        grad_v, grad_W = grad(loss_cl)((v,W))\n",
    "        if(t % freq == 0):\n",
    "            flat_grad  = jnp.concatenate( (grad_v.flatten(), grad_W.flatten()) )\n",
    "            key, split               = random.split(key)\n",
    "            tridiag, lancsoz_vecs    = lanczos_alg(loss_cl, (v,W), 250,  split)\n",
    "            eigs_vals, evecs_tridiag = jnp.linalg.eigh(tridiag)\n",
    "            eig_vecs                 = jnp.dot(evecs_tridiag.T, lancsoz_vecs)\n",
    "            if(eig_vecs.shape[0] < 100):\n",
    "                pad_dim = 100 - eig_vecs.shape[0]\n",
    "                pads = jnp.zeros(shape=(pad_dim,))\n",
    "                eigs_vals = jnp.concatenate( (eigs_vals, pads) )\n",
    "            if(eig_vecs.shape[0] < 15):    \n",
    "                pad_dim = 15 - eig_vecs.shape[0]\n",
    "                pads = jnp.zeros(shape=(pad_dim,dim))\n",
    "                eig_vecs = jnp.concatenate( (eig_vecs, pads) )\n",
    "            for r in range(3):\n",
    "                prior_eig_vecs = eig_vecs_priors[r, :, :]\n",
    "                diff = jnp.linalg.norm( jnp.abs(prior_eig_vecs) - jnp.abs(eig_vecs[-15:,:]) ) \n",
    "                print(\"Diff:\", diff)\n",
    "                eig_vecs_diffs = eig_vecs_diffs.at[(i, t//freq, r)].set(diff)\n",
    "            eig_vecs_priors = eig_vecs_priors.at[2].set(eig_vecs_priors[1, :, :]) \n",
    "            eig_vecs_priors = eig_vecs_priors.at[1].set(eig_vecs_priors[0, :, :]) \n",
    "            eig_vecs_priors = eig_vecs_priors.at[0].set(eig_vecs[-15:,:])\n",
    "            \n",
    "            proj     =   jnp.dot(eig_vecs[-15:,:],flat_grad)\n",
    "            overlap  = jnp.linalg.norm(proj) / (jnp.linalg.norm(flat_grad) + 1e-6)\n",
    "            print(\"Overlap:\", overlap)  \n",
    "            train_loss = loss_cl((v,W))\n",
    "            test_loss  = loss((v,W), data_test, leak)\n",
    "            lin_err    = lin_forward_dyn((v,W), data_test[0], leak)[1]\n",
    "            print('Train loss:', train_loss, 'Test loss:', test_loss, 'Linearization Err:', jnp.max(jnp.abs(lin_err)))\n",
    "            # Save\n",
    "            test_loss_vec  = test_loss_vec.at[(i,t//freq)].set(test_loss)\n",
    "            train_loss_vec = train_loss_vec.at[(i,t//freq)].set(train_loss)\n",
    "            lin_err_vec    = lin_err_vec.at[(i,t//freq)].set(jnp.max(jnp.abs(lin_err)))\n",
    "            overlap_vec    = overlap_vec.at[(i,t//freq)].set(overlap)    \n",
    "            eig_vals_vec   = eig_vals_vec.at[(i,t//freq)].set(eigs_vals[-100:])\n",
    "\n",
    "\n",
    "        v -= learning_rate*grad_v\n",
    "        W -= learning_rate*grad_W \n",
    "        \n",
    "        if(t == T -1):\n",
    "            key, split               = random.split(key)\n",
    "            tridiag, lancsoz_vecs   = lanczos_alg(loss_cl, (v,W), 250,  split)\n",
    "            eig_vals, evecs_tridiag= jnp.linalg.eigh(tridiag)\n",
    "            eig_vecs  = jnp.dot(evecs_tridiag.T, lancsoz_vecs)\n",
    "\n",
    "            if(eig_vecs.shape[0] < 100):\n",
    "                pad_dim = 100 - eig_vecs.shape[0]\n",
    "                pads = jnp.zeros(shape=(pad_dim,))\n",
    "                eig_vals = jnp.concatenate( (eig_vals, pads) )\n",
    "            if(eig_vecs.shape[0] < 15):    \n",
    "                pad_dim = 15 - eig_vecs.shape[0]\n",
    "                pads = jnp.zeros(shape=(pad_dim,dim))\n",
    "                eig_vecs = jnp.concatenate( (eig_vecs, pads) )\n",
    "                \n",
    "            y_hat_init = forward_dyn((v_init, W_init), X_train, leak)\n",
    "            data_hat_init = (X_train, y_hat_init)\n",
    "            loss_cl2 = lambda x: loss(x, data_hat_init, leak)\n",
    "            tridiag2, lancsoz_vecs2 = lanczos_alg(loss_cl2, (v_init,W_init), 250,  split)\n",
    "            eig_vals2, evecs_tridiag2 = jnp.linalg.eigh(tridiag2)\n",
    "            eig_vecs2  = jnp.dot(evecs_tridiag2.T, lancsoz_vecs2)\n",
    "\n",
    "            if(eig_vecs2.shape[0] < 100):\n",
    "                pad_dim = 100 - eig_vecs2.shape[0]\n",
    "                pads = jnp.zeros(shape=(pad_dim,))\n",
    "                eig_vals2 = jnp.concatenate( (eig_vals2, pads) )\n",
    "            if(eig_vecs2.shape[0] < 15):    \n",
    "                pad_dim = 15 - eig_vecs2.shape[0]\n",
    "                pads = jnp.zeros(shape=(pad_dim,dim))\n",
    "                eig_vecs2 = jnp.concatenate( (eig_vecs2, pads) ) \n",
    "                \n",
    "            rel_diffs = jnp.divide( jnp.abs(eig_vals[-100:] - eig_vals2[-100:]), eig_vals2[-100:] + 1e-6)\n",
    "            max_rel_diff = jnp.max(rel_diffs)    \n",
    "            rel_diff_vec = rel_diff_vec.at[i].set(max_rel_diff)\n",
    "            diff = jnp.linalg.norm( jnp.abs(eig_vecs[-15:,:]) - jnp.abs(eig_vecs2[-15:,:]) ) \n",
    "            diff_vec = diff_vec.at[i].set(diff)\n",
    "            print('RelDiff',  max_rel_diff)\n",
    "            print('Diff',  diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc00b403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# np.savez('results12.npz', lin_err0 = lin_err_vec, overlap0 = overlap_vec, test_loss0 = test_loss_vec, train_loss0 = train_loss_vec, eig_vals0 = eig_vals_vec, eig_vecs_diff0 = eig_vecs_diffs, Hg0 = Hg_product_vec)\n",
    "np.savez('approx_res.npz', rel0 = rel_diff_vec, dif0 = diff_vec, test_loss0 = test_loss_vec, train_loss0 = train_loss_vec)\n",
    "print('Done')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
